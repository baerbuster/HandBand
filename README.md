Handband Modules: 
Config Dictionaries and File Loading Input 
Emo-Mathematical Optimization Translation Engine (EMOTE) (Equations and Theory) 
Musical Domain Application (Harmony, Rhythm, Melody, etc.) 
Sonic Domain Application (ADSR, FX, EQ, etc.) 
Sequencer 
Synthesizer 
Mix/Master
Output
Main Orchestrator

Config Dictionaries and File Loading

Input:

	The Input module is HandBand’s sensory interface - the single point where external reality enters the system. Its only job is to read sensor data and normalize it to a standardized range of -1.0 to +1.0, where -1 represents one emotional extreme (calm/low arousal), 0 is neutral baseline, and +1 is the opposite extreme (intense/high arousal). Currently this is a GUI slider, but the architecture is designed so we can swap in biosensors later without touching any other module.

	Input does not interpret what the value means, make musical decisions, or store history. It’s purely mechanical - just accurate sensor reading and normalization. The rest of the system treats it as a black box that provides a single float value. If the sensor fails, it should signal error clearly rather than produce garbage data.

	The evolution path goes from GUI slider to single biosensor (HRV or skin conductance) to multi-sensor fusion, and eventually integrates with Northstar for context-aware calibration. But through all phases, the interface stays simple: get the current state value, check if sensor is connected, and report what type of sensor is active. No downstream module needs to know or care what’s actually providing that number.​​​​​​​​​​​​​​​​

EMOTE:

	EMOTE (Emotional Mathematical Optimization Translation Engine) is HandBand's core intelligence layer that transforms the single emotional state value from Input into multiple abstract dimensions. Its job is nonlinear multi-dimensional mapping - taking that one -1 to +1 number and running it through different mathematical transformation curves to generate independent parameters. Each dimension has its own scaling relationship to the input, meaning an emotional state doesn't translate to proportional changes across all dimensions - some might scale exponentially, others logarithmically, others linearly. EMOTE contains all the pure mathematical functions and music theory primitives needed for these transformations, but it never makes domain-specific decisions about notes or sounds. It outputs abstract numerical values that Musical Domain and Sonic Domain can interpret however they need to.


	EMOTE computes the complete suite of information-theoretic dimensions from the input signal, regardless of which domains will actually use them. These include Shannon's core measures - entropy (uncertainty and information content), redundancy (predictability and compressibility), and mutual information (correlation between signals). Signal characteristics like bandwidth (spread of frequency content), dynamic range (difference between extremes), signal-to-noise ratio, spectral density (distribution across frequencies), and temporal density (distribution across time). Pattern and structure measures including periodicity (regularity of patterns), complexity (shortest description length), self-similarity (fractal dimension), and correlation (dependencies between elements). Rate of change metrics covering first derivative (velocity of change), second derivative (acceleration), and variance or volatility. EMOTE makes no assumptions about which dimensions matter for any given domain or emotional state - it simply computes all of them and provides the full mathematical profile. Domain applications downstream select whichever subset is relevant to their output medium, and eventually the feedback loop discovers through trial and error which dimensions actually drive effective emotional navigation.

	The evolution path for EMOTE moves from hardcoded transformations to learned optimization. Initially, the mathematical relationships between input state and dimensional outputs are manually defined based on intuition and music theory. As the feedback loop closes with biosensor input, EMOTE begins discovering which dimensions and which transformation curves actually succeed at moving users between emotional states. It learns personal calibration - that for a specific user, moving from anxious to focused requires a particular dimensional profile that might differ from another user's optimal profile. Eventually EMOTE integrates with Northstar to access historical context and state definitions, allowing it to understand not just "input is at -0.3" but "user wants to reach their recorded 'deep focus' state and has been trending toward scattered attention for the past hour." The system discovers its own categories and optimal pathways rather than relying on predetermined emotional mappings.

Musical Domain:  Musical Domain is the bridge between EMOTE's abstract information theory and concrete musical decisions. It subscribes to whichever dimensions from EMOTE are relevant to its domain and interprets them through transformation functions. Musical Domain operates in two layers: a Global Layer that interprets EMOTE dimensions to establish key, tonality, chord progressions, tempo, meter, and overall harmonic density, followed by Role-Specific Interpreters that translate this shared musical context into specific parts. The Percussion Interpreter handles rhythmic patterns, accent placement, and groove. The Bass Interpreter constructs basslines and manages root movement. The Harmony Interpreter determines chord voicings, harmonic rhythm, and texture. The Melody Interpreter creates melodic lines, counterpoint, and lead material. Musical Domain only decides WHAT to play - the symbolic structure of the output - without any knowledge of timbre, synthesis, or sound design. It's much closer to the sheet music of the program. It has no knowledge of how things will sound, only what will be played. Its output is purely structural information that Sequencer will organize temporally and Synthesizer will render as actual audio.

	Musical Domain works with scales and modes, chord structures, harmonic progressions, melodic intervals, rhythm patterns, accent patterns, groove patterns, voice leading concepts, tonality, tempo, and meter. These elements combine to create the complete symbolic musical structure that downstream modules will realize as actual sound.

	The evolution path for Musical Domain moves from hardcoded progressions to learned harmonic intelligence. Initially, chord progressions, scale selections, and voice leading rules are manually defined based on music theory intuition. As the feedback loop closes with biosensor input, Musical Domain begins learning which interpretations of EMOTE's dimensional outputs actually succeed at emotional navigation - discovering that certain entropy values map to specific harmonic densities, or that particular periodicity profiles translate to rhythmic patterns that reliably induce target states. The system learns personal harmonic preferences, finding that some users respond to modal interchange while others need simpler diatonic movement. Eventually Musical Domain moves beyond selecting from preset progressions to generating novel harmonic sequences, creating chord progressions and melodic patterns that have never existed but that EMOTE's dimensional profile suggests will work. This evolution happens in coordination with EMOTE's learning - the two layers co-train so Musical Domain's interpretations inform which dimensional profiles EMOTE discovers are effective. Musical Domain integrates with Northstar to understand not just the immediate emotional target but the broader context of user preferences, time of day, and cumulative listening history, allowing it to compose music that serves long-term optimization goals rather than just immediate state changes.

Sonic Domain:

	Sonic Domain is the bridge between Musical Domain's symbolic structure and the actual timbral characteristics of sound. It subscribes to whichever dimensions from EMOTE are relevant to sound design and interprets them through transformation functions. Sonic Domain operates in two layers: a Global Sonic Layer that interprets EMOTE dimensions to establish overall timbral aesthetic, spectral character, and textural density, followed by Role-Specific Sonic Interpreters that apply this shared sonic context to each musical role. The Percussion Sonic Interpreter handles drum timbres, transient shaping, and rhythmic texture. The Bass Sonic Interpreter determines low-frequency character, harmonic content, and sustain characteristics. The Harmony Sonic Interpreter establishes chord timbres, spatial width, and harmonic color. The Melody Sonic Interpreter creates lead timbres, articulation, and expressive shaping. Sonic Domain only decides HOW things sound - the timbral and textural qualities - without knowledge of what notes are being played or when. Its output is synthesis parameter configurations that Synthesizer will use to generate actual audio.

	Sonic Domain works with envelope shaping, filtering and filter modulation, modulation systems, distortion and saturation, spectral processing, waveform selection and mixing, spatial positioning, and dynamic processing. These elements combine to create the complete timbral profile that transforms Musical Domain's symbolic structure into perceptually distinct sound.

	The evolution path for Sonic Domain moves from hardcoded synthesis parameters to learned timbral intelligence. Initially, envelope curves, filter settings, and effect chains are manually defined based on sound design intuition. As the feedback loop closes with biosensor input, Sonic Domain begins learning which interpretations of EMOTE's dimensional outputs actually succeed at emotional navigation - discovering that certain complexity values map to specific filter resonance profiles, or that particular spectral density dimensions translate to waveform mixing ratios that reliably induce target states. The system learns personal timbral preferences, finding that some users respond to warm saturated tones while others need clean precise articulation. Eventually Sonic Domain moves beyond selecting from preset patches to generating novel timbral configurations, creating synthesis parameter combinations that have never existed but that EMOTE's dimensional profile suggests will work. This evolution happens in coordination with both EMOTE and Musical Domain's learning - all three layers co-train so Sonic Domain's timbral realizations inform which musical and dimensional profiles prove effective. Sonic Domain integrates with Northstar to understand not just the immediate sonic target but the broader context of listening environment, time of day, and cumulative exposure, allowing it to shape sound that serves long-term optimization goals rather than just immediate aesthetic preferences.

Sequencer:

	Sequencer is HandBand's universal temporal coordinator, running at audio sample rate to provide the master clock for the entire system. It operates at 44.1kHz, translating between continuous audio time and quantized musical time, calculating which musical step, beat, and measure correspond to each sample count based on tempo and meter information from Musical Domain. It triggers note-on and note-off events at precisely the right sample moments, manages loop wraparound to maintain consistent cycle length, and ensures sample-accurate synchronization across all Synthesizer instances. Sequencer has no interpretive role - it executes timing decisions made by Musical Domain with mathematical precision, providing the infrastructure that allows all other modules to operate in coordinated time.

	Sequencer works with sample-accurate timing calculations, tempo and meter conversion, event scheduling and triggering, loop boundary management, latency compensation, and multi-synthesizer synchronization. These elements combine to translate Musical Domain's abstract temporal structure into the precise sample-by-sample timing that Synthesizer needs to generate continuous audio.


Synthesizer:

Synthesizer is HandBand’s audio rendering engine, instantiated per musical role to generate actual waveforms from symbolic musical information and sonic parameters. Each role has its own Synthesizer instance - Bass Synthesizer, Harmony Synthesizer, Percussion Synthesizer, and Melody Synthesizer - configured for its specific requirements. Synthesizer receives note-on and note-off events from Sequencer specifying when to play, symbolic note information from Musical Domain specifying what to play, and timbral parameters from Sonic Domain specifying how to sound. It handles both synthesized waveform generation and sample-based playback, with each instance configured to use whichever approach suits its role - percussion typically uses sample playback while bass, harmony, and melody use oscillator synthesis, though any combination is possible. It generates audio at a configurable sample rate, managing polyphony and voice allocation as needed for its role. Synthesizer applies sample-level smoothing to all parameter changes to prevent audio artifacts like clicks and pops, interpolating between parameter values according to transition curves specified by Sonic Domain. Synthesizer has no interpretive role - it executes rendering commands with sample-accurate precision, providing the DSP infrastructure that transforms abstract musical and sonic decisions into actual sound waves.

Synthesizer works with oscillator generation and mixing, sample playback and looping, ADSR envelope processing, filter chains and modulation, LFO and FM synthesis, effects processing including distortion and saturation, voice allocation for polyphonic parts, sample-level parameter interpolation, and audio buffer management. These elements combine to render Musical Domain’s symbolic structure with Sonic Domain’s timbral characteristics into continuous audio streams that are mixed and sent to Output.​​​​​​​​​​​​​​​​

Mix/Master:

Mix/Master is HandBand’s audio mixing and mastering module, receiving individual audio streams from all Synthesizer instances and combining them into a final stereo mix. It handles per-role level balancing and panning to create spatial width, summing all role outputs while preventing clipping through gain staging. Mix/Master applies master processing chain including EQ for tonal balance, compression for dynamic control, and limiting for safety. It manages per-role send/return effects like reverb and delay that create shared acoustic space across all parts. Mix/Master operates at the same sample rate as Synthesizer, processing audio buffers in real-time with minimal added latency. It has no interpretive role - it executes mixing decisions and applies professional mastering techniques to ensure the final output is balanced, cohesive, and technically sound.

Mix/Master works with audio stream summing and routing, per-role gain staging and panning, spatial processing and stereo width, master EQ and tonal shaping, multiband compression and limiting, send/return effect chains, metering and level monitoring, and clipping prevention. These elements combine to transform multiple individual Synthesizer outputs into a polished final mix ready for Output delivery.

The evolution path for Mix/Master moves from hardcoded mixing ratios to executing learned mixing intelligence from Global Sonic Layer. Initially, per-role levels, panning positions, master EQ curves, and compression settings are manually defined based on mixing engineering best practices. As the feedback loop closes with biosensor input, Mix/Master begins executing mixing decisions learned by Global Sonic Layer - discovering that certain spectral balance profiles or compression characteristics succeed at emotional navigation for specific users. The system learns personal mixing preferences, finding that some users respond to wider stereo fields and brighter master EQ while others need centered mono sources and warmer tones. Eventually Mix/Master executes sophisticated environment-aware mastering decisions informed by Northstar, adjusting compression ratios for noisy listening environments, compensating master EQ for different playback systems, and adapting overall loudness to time of day and user context. This evolution happens in coordination with Global Sonic Layer’s learning - Mix/Master doesn’t learn independently but rather becomes increasingly sophisticated at executing the mixing vision that Global Sonic Layer develops through experience.​​​​​​​​​​​​​​​​

Output:

Output is HandBand’s audio delivery module, receiving mixed audio streams from all Synthesizer instances and managing their transmission to playback hardware. It handles audio hardware interface configuration, buffer management to minimize latency while preventing dropouts, sample format conversion as needed for the target device, and final volume control. Output maintains consistent timing with Sequencer to ensure sample-accurate delivery, managing the audio driver’s callback system to request the next buffer of samples at precisely the right moments. Output has no interpretive role - it simply delivers the audio stream that Synthesizer has generated with maximum fidelity and minimum latency.

Output works with audio hardware interface management, circular buffer systems, latency compensation and monitoring, sample format conversion between internal processing format and hardware requirements, device selection and configuration, and master volume control. These elements combine to reliably deliver HandBand’s generated audio to the user’s listening environment.

The evolution path for Output expands from audio-only delivery to multi-sensory and conceptual output modalities. Initially handling only audio playback through speakers or headphones, Output evolves to support visual rendering to displays, haptic feedback through vibration or tactile interfaces, and eventually explores olfactory and gustatory outputs where technically feasible. The ultimate vision includes abstract conceptual outputs such as direct neural interfaces or augmented reality overlays that can deliver optimization interventions beyond traditional sensory channels. Each modality follows the same architectural pattern - receiving rendered output from domain-specific synthesis, managing the appropriate hardware interface, and delivering with precise timing coordination. As new output modalities are added, they integrate with Sequencer’s universal timing system to maintain synchronization across all sensory channels, creating coherent multi-modal experiences rather than isolated stimuli.​​​​​​​​​​​​​​​​

Main Orchestrator
